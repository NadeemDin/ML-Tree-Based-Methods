{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a6052a",
   "metadata": {},
   "source": [
    "# Random Forests:\n",
    "\n",
    "- ability to greatly increase the performance based on expanding ideas from the Decision Tree.\n",
    "- known as Ensemble learners since they rely on an ensemble of models/multiple decision trees. \n",
    "\n",
    "### Motivation behind RFs: \n",
    "\n",
    "- Decision Tree restricted by Gini Impurity and single DT always has same root node and so DT will always be the same and thus limited. \n",
    "- Potential to overfit to data\n",
    "- Splitting Criteria can lead to some features never being used. \n",
    "- RF : create subsets of randomly selected n features at each potential split \n",
    "\n",
    "- each tree 'votes' on y label and label with most 'votes' is selected. (we know probability of being correct)\n",
    "- can average out predictive continous output.\n",
    "\n",
    "### Sklearns Random Forest class call and its hyperparameters:\n",
    "\n",
    "#### 1. n_estimators:\n",
    "    - the more DTs we have the more oppurtunity to learn from a variety of feature subset combinations\n",
    "    - n_of trees:\n",
    "        - n_estimators documentation suggests 64-128 trees, CV grid search of trees, plot error vs number of trees \n",
    "        (see elbow KNN method, will notice a diminishing error reduction after some N trees.\n",
    "        - after a while, new trees will effectively be duplicates of eachother and so dont really cause much of an issue.\n",
    "        - over fitting is thus of minimal concern\n",
    "    - Default value is 100. Gridsearch for better parameter. \n",
    "       \n",
    "#### 2. max_features\n",
    "    - how many? publication says log(base2)(N+1) where N is total features.\n",
    "    - number of features in subset? sqrt(N) or N/3 for regression tasks (larger than sqrt(N))\n",
    "    - best practice to gridsearch from sqrt(N) and then adjust accordingly \n",
    "\n",
    "#### 3. bootstrap samples\n",
    "    - in general terms: describes random sampling with replacement (random sampling where a feature can be selected more than once) \n",
    "    - help create more diverse trees that are less likely to be correlated with eachother.\n",
    "    - two randomised training componenets: 1. subset of features used, 2. boostrap rows of data (randomises rows)\n",
    "    - bootstrapping used to reduce correlation, helps model generalise better on data it hasnt seen before.\n",
    "    - \n",
    "    \n",
    "    \n",
    "#### 4. oob_score (out of bag error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985e487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba17e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
